{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de textos\n",
    "Juan Manuel Silva y Cristian Sarmiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     certificate chain too long (_ssl.c:1006)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     certificate chain too long (_ssl.c:1006)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice’s Adventures in Wonderland\n",
      "\n",
      "by Lewis Carroll\n",
      "\n",
      "THE MILLENNIUM FULCRUM EDITION 3.0\n",
      "\n",
      "Contents\n",
      "\n",
      " CHAPTER I.     Down the Rabbit-Hole\n",
      " CHAPTER II.    The Pool of Tears\n",
      " CHAPTER III.   A Caucus-Race and a Long Tale\n",
      " CHAPTER IV.    The Rabbit Sends in a Little Bill\n",
      " CHAPTER V.     Advice from a Caterpillar\n",
      " CHAPTER VI.    Pig and Pepper\n",
      " CHAPTER VII.   A Mad Tea-Party\n",
      " CHAPTER VIII.  The Queen’s Croquet-Ground\n",
      " CHAPTER IX.    The Mock Turtle’s Story\n",
      " CHAPTER X.     The Lobster Quadrille\n",
      " CHAPTER XI.    Who Stole the Tarts?\n",
      " CHAPTER XII.   Alice’s Evidence\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     certificate chain too long (_ssl.c:1006)>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "## Lectura del archivo\n",
    "with open('alice_in_wonderland.txt', 'r', encoding='utf-8') as f:\n",
    "    contents = f.read()\n",
    "    \n",
    "## primera visualización de los datos:\n",
    "print(contents[835:1400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocesamiento básico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Eliminación de mayúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the project gutenberg ebook of alice's adventures in wonderland\n",
      "    \n",
      "this ebook is for the use of a\n"
     ]
    }
   ],
   "source": [
    "contents = contents.lower()\n",
    "print(contents[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'alice', 's', 'adventures', 'in', 'wonderland']\n"
     ]
    }
   ],
   "source": [
    "## Se extraen las palabras de los contenidos usando tokenización de \n",
    "tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n",
    "tokens = tokenizer.tokenize(contents)\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total palabras:  30591\n",
      "Total unicas:  3143\n"
     ]
    }
   ],
   "source": [
    "print('Total palabras: ', len(tokens))\n",
    "print('Total unicas: ', len(set(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "['project', 'gutenberg', 'ebook', 'alice', 'adventures', 'wonderland', 'ebook', 'use', 'anyone', 'anywhere']\n"
     ]
    }
   ],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "print(stop_words[:10])\n",
    "\n",
    "words_filtered = [word for word in tokens if word not in stop_words]\n",
    "print(words_filtered[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total palabras quitando stopwords:  14298\n",
      "Total unicas quitando stopwords:  2998\n"
     ]
    }
   ],
   "source": [
    "print('Total palabras quitando stopwords: ', len(words_filtered))\n",
    "print('Total unicas quitando stopwords: ', len(set(words_filtered)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Eliminación de abreviaciones y acentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Eliminacion de acentos'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Eliminación de acentos'\n",
    "unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = unicodedata.normalize('NFKD', contents).encode('ascii', 'ignore').decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Lematización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'ebook', 'alice', 'adventure', 'wonderland', 'ebook', 'use', 'anyone', 'anywhere', 'united', 'state', 'part', 'world', 'cost']\n"
     ]
    }
   ],
   "source": [
    "# Inicializar el lematizador\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Matizar cada palabra\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words_filtered]\n",
    "\n",
    "print(lemmatized_words[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total palabras lematizadas:  14298\n",
      "Total unicas lematizadas:  2772\n"
     ]
    }
   ],
   "source": [
    "print('Total palabras lematizadas: ', len(lemmatized_words))\n",
    "print('Total unicas lematizadas: ', len(set(lemmatized_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Steaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'ebook', 'alic', 'adventur', 'wonderland', 'ebook', 'use', 'anyon', 'anywher', 'unit', 'state', 'part', 'world', 'cost']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in words_filtered]\n",
    "\n",
    "print(stemmed_words[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total palabras steam:  14298\n",
      "Total unicas steam:  2278\n"
     ]
    }
   ],
   "source": [
    "print('Total palabras steam: ', len(stemmed_words))\n",
    "print('Total unicas steam: ', len(set(stemmed_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Técnicas de representación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras: ['000' '11' '1500' '1887' '20' '2001' '2008' '2024' '27' '30' '50' '501'\n",
      " '596' '60' '6221541' '64' '801' '809' '84116' '90' '_alice' '_all'\n",
      " '_all_' '_and' '_are_' '_at' '_before' '_beg_' '_began_' '_best_' '_can_'\n",
      " '_could' '_could_' '_curtseying_' '_don' '_ever_' '_everybody_' '_fit_'\n",
      " '_hated_' '_have_' '_he_' '_her_' '_here_' '_him_' '_his_' '_how' '_i'\n",
      " '_i_' '_in_' '_inside_' '_is_' '_it' '_it_' '_less_' '_little_' '_me_'\n",
      " '_mine_' '_more_' '_must_' '_myself_' '_never_' '_no_' '_not' '_not_'\n",
      " '_one_' '_ours_' '_outside_' '_please_' '_plenty_' '_poison_'\n",
      " '_precious_' '_proves_' '_quite_' '_red_' '_said' '_said_' '_she' '_she_'\n",
      " '_sit_' '_some_' '_somebody_' '_something_' '_somewhere_' '_speaker_'\n",
      " '_stolen' '_that' '_that_' '_their_' '_then_' '_there_' '_these' '_they'\n",
      " '_think_' '_this' '_this_' '_through_' '_tis' '_took' '_turtle'\n",
      " '_twinkle']\n",
      "Bag of Words:\n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(lemmatized_words)\n",
    "\n",
    "bow_array = X.toarray()\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Resultado\n",
    "print(\"Palabras:\", feature_names[:100])\n",
    "print(\"Bag of Words:\\n\", bow_array[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', 'gutenberg'),\n",
       " ('gutenberg', 'ebook'),\n",
       " ('ebook', 'alice'),\n",
       " ('alice', 'adventure'),\n",
       " ('adventure', 'wonderland'),\n",
       " ('wonderland', 'ebook'),\n",
       " ('ebook', 'use'),\n",
       " ('use', 'anyone'),\n",
       " ('anyone', 'anywhere'),\n",
       " ('anywhere', 'united')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "bigram_list = list(bigrams(lemmatized_words))\n",
    "bigram_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Term Frequency-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz TF-IDF:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Términos:\n",
      " ['000' '11' '1500' ... 'youth' 'zealand' 'zigzag']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(lemmatized_words)\n",
    "\n",
    "# Mostrar la matriz TF-IDF\n",
    "print(\"Matriz TF-IDF:\\n\", tfidf_matrix.toarray())\n",
    "\n",
    "# Mostrar los términos\n",
    "print(\"Términos:\\n\", vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Técnicas de representación adicionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector para 'alice':\n",
      " [-0.41106218  0.5566622   0.07799479  0.07226732  0.17316268 -0.8077865\n",
      "  0.07842701  0.969804   -0.61568147 -0.51299405  0.01843098 -0.6103687\n",
      " -0.39528337  0.31936285 -0.03298039 -0.30175847  0.20804974 -0.08221999\n",
      " -0.12310431 -0.96926713 -0.07051158  0.2594048   0.16562517 -0.45268953\n",
      " -0.32171124  0.08117118 -0.37572294 -0.53216845 -0.29094487  0.33375967\n",
      "  0.73074555 -0.24172832  0.25742027 -0.61648834 -0.22345953  0.7510103\n",
      " -0.10204238 -0.25358525 -0.12527071 -0.8315402   0.29418647 -0.47284278\n",
      " -0.0396327   0.25796854  0.40965822  0.04216215 -0.24819313 -0.3133194\n",
      "  0.01558769  0.24121435  0.3692049  -0.2222646   0.25920743 -0.0865428\n",
      " -0.5565296   0.246616    0.08230811 -0.08938461 -0.26653388  0.04362943\n",
      "  0.01691168  0.03610986  0.28622842 -0.0209935  -0.36599684  0.37502635\n",
      "  0.32099614  0.4826203  -0.89844006  0.6007489  -0.21620585  0.64274967\n",
      "  0.6853746  -0.00388802  0.7212434   0.1303567  -0.267348    0.0743857\n",
      " -0.47566378  0.03929089 -0.33745173 -0.00310475 -0.5082126   0.5640362\n",
      " -0.26977292 -0.12002232  0.44715565  0.33188662  0.5265656   0.39277762\n",
      "  0.6559819   0.00359601  0.14354965 -0.10425503  0.651256    0.48494008\n",
      "  0.25852254 -0.4121611   0.24419527 -0.04923135]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "## Generar las frases tokenizadas\n",
    "sentences = [tokenizer.tokenize(sentence.strip()) for sentence in contents.split('\\n') \n",
    "             if sentence.strip() != '']\n",
    "\n",
    "# Entrenar el modelo Word2Vec\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Obtener el vector para una palabra específica (por ejemplo, \"alice\")\n",
    "vector = model.wv['alice']\n",
    "\n",
    "print(\"Vector para 'alice':\\n\", vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove import Corpus, Glove\n",
    "\n",
    "# Crear un corpus\n",
    "corpus = Corpus()\n",
    "corpus.fit(sentences, window=10)\n",
    "\n",
    "# Create and train the GloVe model\n",
    "glove = Glove(no_components=100, learning_rate=0.05)\n",
    "glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tabla de análisis\n",
    "\n",
    "![Example Image](tabla_resumen.png)\n",
    "\n",
    "Las técnicas de preprocesamiento, como la normalización, tokenización y eliminación de stop words, son esenciales para transformar el texto en un formato que los modelos de NLP puedan entender. Esto mejora la calidad de los datos y la precisión de los resultados obtenidos.\n",
    "\n",
    "Métodos como Bag of Words (BoW) y bigramas son fáciles de implementar, pero ignoran el contexto y las relaciones semánticas profundas entre las palabras. Esto puede resultar en una pérdida significativa de información, lo que limita la efectividad del modelo en tareas más complejas.\n",
    "\n",
    "Técnicas más avanzadas como Word2Vec y GloVe ofrecen representaciones densas que capturan relaciones semánticas y sintácticas, mejorando así la comprensión del lenguaje. Sin embargo, requieren grandes volúmenes de datos para entrenar y pueden ser computacionalmente intensivas, lo que plantea desafíos en su implementación."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c06e3e46abf38078fe4dac36a0085ec2b134ebbd73dd076183d243eeca6918f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
